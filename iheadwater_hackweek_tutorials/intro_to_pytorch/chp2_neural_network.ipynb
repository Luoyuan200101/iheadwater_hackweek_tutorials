{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b24b7836-2a07-4b80-abcb-b12c51aba2f2",
   "metadata": {},
   "source": [
    "# 再探torch.nn\n",
    "pytorch提供了很多设计良好的modules和classed，torch.nn，torch.optim，和Dataset以及Dataloader 来帮助构建和训练神经网络。为了充分利用它们的能力并为自己的问题定制，需要真正地理解它们都在做什么。一个训练神经网络训练识别MNIST数据集的例子。首先只使用最基本的tensor的功能，然后逐步添加torch.nn, torch.optim, Dataset, or DataLoader的特性进去，展示每一块究竟在做什么，以及怎么能让代码更加简洁灵活。\n",
    "\n",
    "## 数据集设置\n",
    "MNIST数据集。用pathlib处理路径。使用requests下载数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6a46c8-aac8-45d5-87db-3d5cdad34c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"http://deeplearning.net/data/mnist/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "        content = requests.get(URL + FILENAME).content\n",
    "        (PATH / FILENAME).open(\"wb\").write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d782b48e-69ed-4b29-8d0e-a1421ab9faf9",
   "metadata": {},
   "source": [
    "数据集是numpy数组格式，已经用pickle序列化存储。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a46dab-d2e6-4889-866f-cdb6bff6497b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b75f19-5dcb-49e4-91e1-5db420ec91bf",
   "metadata": {},
   "source": [
    "每个图片都是28×28的，展开为一维784 长度存储的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc87f027-1f6b-4a3b-9e23-936ebedec9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "\n",
    "pyplot.imshow(x_train[0].reshape((28, 28)), cmap=\"gray\")\n",
    "print(x_train.shape)\n",
    "pyplot.show()\n",
    "(50000, 784)\n",
    "<Figure size 640x480 with 1 Axes>\n",
    "PyTorch 使用torch.tensor，而不是numpy数组。所以我们需要转换我们的数据。\n",
    "\n",
    "import torch\n",
    "\n",
    "x_train, y_train, x_valid, y_valid = map(\n",
    "    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
    ")\n",
    "n, c = x_train.shape\n",
    "x_train, x_train.shape, y_train.min(), y_train.max()\n",
    "print(x_train, y_train)\n",
    "print(x_train.shape)\n",
    "print(y_train.min(), y_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c8d2b2-4995-40f2-a8fb-c9fb7beed448",
   "metadata": {},
   "source": [
    "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
    "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
    "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
    "        ...,\n",
    "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
    "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
    "        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])\n",
    "torch.Size([50000, 784])\n",
    "tensor(0) tensor(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3548060-a30f-4403-8c60-aed6b0af1eb2",
   "metadata": {},
   "source": [
    "## 不使用torch.nn构建神经网络\n",
    "\n",
    "创建随机tensors，表示权重和偏置，然后告诉这些tensor，它们需要梯度，这就会让PyTorch 记录在tensors上的运算，构建计算图，以使能自动进行反向传播计算。因为初始化这步我们是不想要加入计算图的，所以requires_grad要在初始化之后设置。注意尾部加上_表示变量原地执行该操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d79dcc-c326-476d-bdc7-14fe7c53cc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "weights = torch.randn(784, 10) / math.sqrt(784)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf4a0e8-ea20-4503-bd02-f7d6bc2dc4eb",
   "metadata": {},
   "source": [
    "因为pytorch自动计算梯度的能力，我们可以使用标准的python函数作为model的部分。虽然pytorch准备了很多现成的函数，但到目前为止还是可以轻松地实现自己的版本。pytorch会自动使用GPU加速。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3765ba3-6074-4ead-a1bc-2e7dcda55e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
    "\n",
    "def model(xb):\n",
    "    return log_softmax(xb @ weights + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a051df-da4a-439d-b8e2-b69e8887f871",
   "metadata": {},
   "source": [
    "@表示点乘运算，接下来调用函数作用到一个batch的数据上，这称为one forward pass。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04227334-6153-449d-b95b-6b671c587325",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64  # batch size\n",
    "\n",
    "xb = x_train[0:bs]  # a mini-batch from x\n",
    "preds = model(xb)  # predictions\n",
    "preds[0], preds.shape\n",
    "print(preds[0], preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0097d8e-6b4c-4cbf-a430-3cb933bb1bbf",
   "metadata": {},
   "source": [
    "tensor([-2.0446, -1.9517, -2.3846, -2.6700, -1.7815, -2.9094, -2.7893, -2.2573,\n",
    "        -2.6186, -2.2513], grad_fn=<SelectBackward>) torch.Size([64, 10])\n",
    "preds张量不仅包含张量值，也有梯度函数，这会被用来做反向传播。接下来还是直接使用python函数做loss计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1e93a8-7f19-4827-9989-9584d5839dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(input, target):\n",
    "    return -input[range(target.shape[0]), target].mean()\n",
    "\n",
    "loss_func = nll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6bf173-ff70-4830-93a4-a791558f54d6",
   "metadata": {},
   "source": [
    "现在看看目前的loss是多少。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4df3ef3-7062-4b05-88dc-dc99096fb59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "yb = y_train[0:bs]\n",
    "print(loss_func(preds, yb))\n",
    "tensor(2.2833, grad_fn=<NegBackward>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c90f6db-2845-4bb3-9113-d9a8850fbe70",
   "metadata": {},
   "source": [
    "现在可以看看模型的精度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a70ed9a-a7bd-429e-9e55-ea63454c5033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()\n",
    "print(accuracy(preds, yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc963a79-a5b7-48c9-96b1-d8a8b77f9795",
   "metadata": {},
   "source": [
    "tensor(0.1094)\n",
    "一次的训练到预测的过程搞定之后，现在可以执行循环，每次迭代的内容是："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ef230b-afb4-4384-afa9-c1f7d07f2863",
   "metadata": {},
   "source": [
    "- select a mini-batch of data (of size bs)\n",
    "- use the model to make predictions\n",
    "- calculate the loss\n",
    "- loss.backward() updates the gradients of the model, in this case, weights and bias.\n",
    "在torch.no_grad()环境下执行更新参数的过程。然后在下次loop之前，将梯度设置为0。否则的话梯度会自动累计。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe7187f-3731-41a0-b5d8-b45b99481970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "lr = 0.5  # learning rate\n",
    "epochs = 2  # how many epochs to train for\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        #         set_trace()\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * lr\n",
    "            bias -= bias.grad * lr\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5422a6-df00-4c3f-818c-e3089a59fe78",
   "metadata": {},
   "source": [
    "现在我们已经完整地写了一个最小的神经网络了。可以看下现在的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2470e6-71a1-43ad-9412-61d420103d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810f8414-673e-4539-9daa-5a345f1898ec",
   "metadata": {},
   "source": [
    "tensor(0.0799, grad_fn=<NegBackward>) tensor(1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc3337c-d550-441f-941e-9742f30c9ba2",
   "metadata": {},
   "source": [
    "## 使用 torch.nn.functional\n",
    "\n",
    "现在开始重构代码，使用nn包。每步都使我们的代码更简洁更灵活更易理解。\n",
    "\n",
    "首先就是用torch.nn.functional代替我们手写的激活函数和损失函数。该module下有很多函数可以使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a3e8a8-c31e-4a1e-a6ee-e7acb4f42276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "def model(xb):\n",
    "    return xb @ weights + bias\n",
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6c9a27-2a15-43e2-aabe-4dbe4e19a85a",
   "metadata": {},
   "source": [
    "tensor(0.0799, grad_fn=<NllLossBackward>) tensor(1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac455e9-5cf7-492a-8770-5ff6d7ec6667",
   "metadata": {},
   "source": [
    "## 使用nn.Module\n",
    "接下来使用 nn.Module and nn.Parameter，继承nn.Module类，获取权重和偏置并做前向计算。nn.Module有很多属性和方法可以使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d2d2a4-4c4f-475d-b46e-383dee0c55c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n",
    "        self.bias = nn.Parameter(torch.zeros(10))\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return xb @ self.weights + self.bias\n",
    "    \n",
    "model = Mnist_Logistic()\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce173355-fc4c-4ec9-baea-2af53f99017e",
   "metadata": {},
   "source": [
    "tensor(2.2756, grad_fn=<NllLossBackward>)\n",
    "\n",
    "现在利用model.parameters()和model.zero_grad()，使前面的参数更新过程更加简洁，并且更不容易漏掉某些参数。比如："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2487aebd-a41a-4355-82f3-3cdca75a87fa",
   "metadata": {},
   "source": [
    "with torch.no_grad():\n",
    "    for p in model.parameters(): p -= p.grad * lr\n",
    "    model.zero_grad()\n",
    "现在将训练loop放入fit函数，并运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaeca04-8a06-42ed-865f-e42706bed01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n - 1) // bs + 1):\n",
    "            start_i = i * bs\n",
    "            end_i = start_i + bs\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= p.grad * lr\n",
    "                model.zero_grad()\n",
    "\n",
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbfe9f1-386e-4ebe-b3fe-bc5cc238aca7",
   "metadata": {},
   "source": [
    "看看现在的loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31f7bdc-3099-4b42-8b62-cc6e3acb56aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57579612-0e97-43db-bfbe-86e7bdd0f880",
   "metadata": {},
   "source": [
    "tensor(0.0817, grad_fn=<NllLossBackward>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956b8d4e-b657-41b1-b95b-c601dc50def4",
   "metadata": {},
   "source": [
    "## 使用nn.Linear\n",
    "\n",
    "使用nn.Linear定义linear层，替代 xb @ self.weights + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea320cb-5d31-4305-9042-e4be033def86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(784, 10)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.lin(xb)\n",
    "    \n",
    "model = Mnist_Logistic()\n",
    "print(loss_func(model(xb), yb))\n",
    "fit()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d8a95b-cbf4-4cb4-8740-8b1399c88695",
   "metadata": {},
   "source": [
    "tensor(2.3328, grad_fn=<NllLossBackward>)\n",
    "tensor(0.0814, grad_fn=<NllLossBackward>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d3d8e6-9e21-4f61-8c9c-774936723958",
   "metadata": {},
   "source": [
    "## 使用optim\n",
    "\n",
    "torch.optim有很多优化函数。可用它来执行权重更新。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da59f2a3-4fcf-4e51-99fc-edb32cac71f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "def get_model():\n",
    "    model = Mnist_Logistic()\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "model, opt = get_model()\n",
    "print(loss_func(model(xb), yb))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5357e9f-8db9-4c1f-9593-1c9c9ab38f77",
   "metadata": {},
   "source": [
    "tensor(2.3390, grad_fn=<NllLossBackward>)\n",
    "tensor(0.0804, grad_fn=<NllLossBackward>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845ea6cd-ca60-408a-84ac-3b0ec324e01a",
   "metadata": {},
   "source": [
    "## 使用Dataset\n",
    "\n",
    "定义一个子类FacialLandmarkDataset 继承Dataset，重写len函数和getitem函数。先看看pytorch的TensorDataset，它是一个包含tensors的Dataset。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a6e0cb-cb6f-4b07-93f7-aa3f000c2282",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "train_ds = TensorDataset(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9446fe71-c6b0-4ca2-8632-ca585bf7c489",
   "metadata": {},
   "source": [
    "可以直接使用对象的索引得到x和y，而不需要再像下面这样分别索引："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a8ce3e-bde0-4ae9-abb1-5cd1082d20a4",
   "metadata": {},
   "source": [
    "xb = x_train[start_i:end_i]\n",
    "yb = y_train[start_i:end_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05794ae-dbc7-480f-b6ad-5f36562aae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = train_ds[i*bs : i*bs+bs]\n",
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        xb, yb = train_ds[i * bs: i * bs + bs]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3eff7e-4fdd-4e7c-a5f8-e8bc806f123c",
   "metadata": {},
   "source": [
    "tensor(0.0827, grad_fn=<NllLossBackward>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776edca0-fff9-4f27-ae97-ed320ccb78ad",
   "metadata": {},
   "source": [
    "## 使用DataLoader\n",
    "DataLoader 是用来管理batches的，可以从Dataset创建一个DataLoader，它会使数据batches的循环更容易。DataLoader自动地给出每个minibatch。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdf7bd6-fd81-4768-ba56-188b08847220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d580fe3-daf6-453e-b4ab-d40cf878f603",
   "metadata": {},
   "source": [
    "之前像这样loop：\n",
    "\n",
    "for i in range((n-1)//bs + 1):\n",
    "    xb,yb = train_ds[i*bs : i*bs+bs]\n",
    "    pred = model(xb)\n",
    "现在可以这样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cbdd1f-b851-4d1e-918e-87f9073b5d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 直接取xb, yb\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2893ee0c-c18f-4cf3-abf0-e58ebd4b487c",
   "metadata": {},
   "source": [
    "tensor(0.0825, grad_fn=<NllLossBackward>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58244348-3716-4caf-9f33-12434c44fca6",
   "metadata": {},
   "source": [
    "## 添加验证集\n",
    "\n",
    "为了防止过拟合，验证数据集是必要的。打乱训练数据对防止batches间的相关性和过拟合是很重要的。验证集打不打乱结果是一样的，所以没必要shuffle。\n",
    "\n",
    "验证集的batchsize设置比训练集大，因为验证集不需要反向传播，因此需要更少的memory。所以这里用两倍的训练集batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62f34a1-1658-41a7-a57c-1c25db1f9c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "\n",
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=bs * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd834140-3665-4004-aaf6-fb945aeabdba",
   "metadata": {},
   "source": [
    "现在在每个epoch最后来计算验证集loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107bd00a-21f6-4bef-9818-f417627e4e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = sum(loss_func(model(xb), yb) for xb, yb in valid_dl)\n",
    "\n",
    "    print(epoch, valid_loss / len(valid_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c67e8dd-97a1-4af9-aa6e-ad833a1dbf77",
   "metadata": {},
   "source": [
    "0 tensor(0.3152)\n",
    "1 tensor(0.3378)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16a8df3-d31e-4021-b690-0e2d3b8530c1",
   "metadata": {},
   "source": [
    "注意，我们总是在训练之前调用model.train()，这是因为后面会在推断之前调用model.eval()，这时候模型会进入验证模式，所以重新训练之前要先切换回训练模式。它们是用来确保不同的阶段行为是适当的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e3264c-422d-498c-9290-4962339a56eb",
   "metadata": {},
   "source": [
    "## 构建fit() 和 get_data()\n",
    "\n",
    "专门写一个loss函数计算每个batch的loss，loss_batch。\n",
    "\n",
    "前向计算的时候需要optimizer，反向的时候是不需要的。\n",
    "\n",
    "然后构建一个fit函数，还有get_data。这样我们就能非常容易地执行我们的代码了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62465ee3-ed38-460e-98b3-ee77ded3eb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
    "            )\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "\n",
    "        print(epoch, val_loss)\n",
    "        \n",
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=bs * 2),\n",
    "    )\n",
    "\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "model, opt = get_model()\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f303f797-16c6-46b4-b2cc-0ad40c80a39e",
   "metadata": {},
   "source": [
    "0 0.338123899435997\n",
    "1 0.30423860963582994"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2c53b2-4c19-499f-ab56-23822bf10fc1",
   "metadata": {},
   "source": [
    "## 转换为CNN\n",
    "三个卷积层，前面的函数都没有指定模型形式，所以可以直接拿来用以训练 CNN 模型。使用pytorch预定义的Conv2d 类作为卷积层。用三层卷积层定义CNN。每个卷积后是一个ReLU，最后执行一次平均池化，关于CNN的内容可以参考后面 5-cnn-example 文件夹中的文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c79ba3-bf72-46ef-a374-81d98c6412d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        xb = xb.view(-1, 1, 28, 28)\n",
    "        xb = F.relu(self.conv1(xb))\n",
    "        xb = F.relu(self.conv2(xb))\n",
    "        xb = F.relu(self.conv3(xb))\n",
    "        xb = F.avg_pool2d(xb, 4)\n",
    "        return xb.view(-1, xb.size(1))\n",
    "\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e1a2ec-2652-44b8-8c63-521ac018c729",
   "metadata": {},
   "source": [
    "view 函数就是 pytorch版本的numpy的reshape函数。Momentum 是随机梯度下降的一种变形，它考虑到以前的更新，通常会使得训练更快。因此使用它："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5409551-a3d3-414f-b842-7ceae883d47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Mnist_CNN()\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c898716e-da5b-46d4-936f-d6276b4acbe8",
   "metadata": {},
   "source": [
    "0 0.4330894303798676\n",
    "1 0.2754692686676979"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f055608-aefb-4c59-b566-b31805458bf2",
   "metadata": {},
   "source": [
    "## nn.Sequential\n",
    "\n",
    "torch.nn 中还有一个很好用的类，Sequential 。Sequential 对象会以序列的形式运行它之中的每个module。这是一种更简单地编写神经网络的方式。为了利用sequential，对给定的函数，需要继承 nn.Module 定义定制的层。比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7badd989-5298-4f47-8212-30fd64b3160a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "\n",
    "\n",
    "def preprocess(x):\n",
    "    return x.view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d544e93f-1bdb-4d59-9e0c-49738e9be2c8",
   "metadata": {},
   "source": [
    "然后利用该层，就可以构建Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fa6cd7-1364-4298-872f-8a0331669e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    Lambda(preprocess),\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(4),\n",
    "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
    ")\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1954da-d79a-42cb-a46d-325adf1ecd19",
   "metadata": {},
   "source": [
    "0 0.3398027998447418\n",
    "1 0.2788183623671532"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ac70a8-7caf-46bf-825d-657ebfdcfce3",
   "metadata": {},
   "source": [
    "## 包装DataLoader\n",
    "现在CNN已经很简洁了，不过它还只能用到MNIST上，因为：\n",
    "\n",
    "- 假设了输入是 28*28 向量\n",
    "- 假设了最后的CNN grid size是 4*4 （平均池化 kernel size）\n",
    "\n",
    "现在如果取消这两个假设，让模型可以应对任意的 2d 单 channel 图片。首先要除掉初始的Lambda层，将数据处理移到一个generator中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d51412-dc95-4d8e-be65-584864123fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    return x.view(-1, 1, 28, 28), y\n",
    "\n",
    "\n",
    "class WrappedDataLoader:\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "    def __iter__(self):\n",
    "        batches = iter(self.dl)\n",
    "        for b in batches:\n",
    "            yield (self.func(*b))\n",
    "\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
    "valid_dl = WrappedDataLoader(valid_dl, preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d41929-3289-4775-a9e2-9fd7a2b4c26d",
   "metadata": {},
   "source": [
    "可以使用 nn.AdaptiveAvgPool2d 替换 nn.AvgPool2d ， 这允许我们定义输出tensor的大小为我们想要的而不是我们现在有的。现在就可以这样了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bf4e1e-55e3-4f95-a601-47102af61b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
    ")\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a3d57b-c63b-48d8-8faf-2ae86352e0e5",
   "metadata": {},
   "source": [
    "0 0.3950530921459198\n",
    "1 0.24604604278206826"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a43e5c0-0726-4681-bd7a-07e5ec770582",
   "metadata": {},
   "source": [
    "## 使用GPU\n",
    "\n",
    "首先检查自己的GPU是否可用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1acaf3f-f86d-4424-86f1-c9f99e9c156a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0937a427-f9c5-4428-a953-bf47b7b787b4",
   "metadata": {},
   "source": [
    "False\n",
    "\n",
    "然后可以创建一个设备对象："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991d8bcb-ce02-4c22-9322-3d222ab0bf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c787fb-71a7-4f79-8cd0-c3bf1a52b50a",
   "metadata": {},
   "source": [
    "将数据放入设备对象中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaeb3cf-28d6-43fb-a6e4-770daabf179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    return x.view(-1, 1, 28, 28).to(dev), y.to(dev)\n",
    "\n",
    "\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
    "valid_dl = WrappedDataLoader(valid_dl, preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ea74d8-0a44-4f31-b197-0d33f4775fdc",
   "metadata": {},
   "source": [
    "模型也放入设备中，就可以快速计算了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8887688-ce03-49ba-89a2-3b326e9934dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(dev)\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce53a002-d535-4b35-88ae-d91d99849da6",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "在模型训练中，dropout是个比较重要的环节，以下给出一个示例。\n",
    "\n",
    "参考：Dropout在RNN中的应用综述\n",
    "\n",
    "受性别在进化中的作用的启发，Hinton等人最先提出Dropout，即暂时从网络中移除神经网络中的单元。 Srivastava等人将Dropout应用于前馈神经网络和受限玻尔兹曼机，其工作的核心概念是“在加入了dropout的神经网络在训练时，每个隐藏单元必须学会随机选择其他单元样本。这理应使每个隐藏的单元更加健壮，并驱使它自己学到有用的特征，而不依赖于其他隐藏的单元来纠正它的错误“。\n",
    "\n",
    "在标准神经网络中，每个参数通过梯度下降逐渐优化达到全局最小值。因此，隐藏单元可能会为了修正其他单元的错误而更新参数。这可能导致“共适应”，反过来会导致过拟合。而假设通过使其他隐藏单元的存在不可靠，dropout阻止了每个隐藏单元的共适应。\n",
    "\n",
    "随着数据集变大，dropout的增益增加到一个点然后下降。 这表明，对于任何给定的网络结构和dropout率，存在一个“最佳点”。\n",
    "\n",
    "用伯努利分别来表示隐藏单元被激活的概率，其中值1为概率p，否则为0。\n",
    " \n",
    "\n",
    "代码如下所示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f3d53c-f333-482c-9edd-784088b062d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout():\n",
    "    def __init__(self, prob=0.5):\n",
    "        self.prob = prob\n",
    "        self.params = []\n",
    "    def forward(self,X):\n",
    "        self.mask = np.random.binomial(1,self.prob,size=X.shape) / self.prob\n",
    "        out = X * self.mask\n",
    "    return out.reshape(X.shape)\n",
    "    def backward(self,dout):\n",
    "        dX = dout * self.mask\n",
    "        return dX,[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a84e18-4a35-4ab4-8f62-0866dee5a97f",
   "metadata": {},
   "source": [
    "在Dropout之后，Wan等人的进一步提出了DropConnect，它“通过随机丢弃权重而不是激活来扩展Dropout”。 “使用Drop Connect, drop的是每个连接，而不是每个输出单元。”与Dropout一样，该技术仅适用于全连接层。\n",
    "\n",
    "通过将dropout应用于输入权重而不是激活，DropConnect可以推广到全连接网络层的整个连接结构。\n",
    "\n",
    "有dropout的神经网络和没有的区别：\n",
    "\n",
    "一个是在神经网络结构中增加dropout层；另一个，注意dropout有两个mode，一是对train data的，一个是对test data的。在train data中进行dropout，但是在test data中是不用的\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de3b689-68cd-4601-84a3-0c53584f930b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://deeplearningcourses.com/c/data-science-deep-learning-in-theano-tensorflow\n",
    "# https://www.udemy.com/data-science-deep-learning-in-theano-tensorflow\n",
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "# Note: is helpful to look at keras_example.py first\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from util import get_normalized_data\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "\n",
    "# get the data, same as Theano + Tensorflow examples\n",
    "# no need to split now, the fit() function will do it\n",
    "Xtrain, Xtest, Ytrain, Ytest = get_normalized_data()\n",
    "\n",
    "# get shapes\n",
    "_, D = Xtrain.shape\n",
    "K = len(set(Ytrain))\n",
    "\n",
    "# Note: no need to convert Y to indicator matrix\n",
    "\n",
    "\n",
    "# the model will be a sequence of layers\n",
    "model = torch.nn.Sequential()\n",
    "\n",
    "# ANN with layers [784] -> [500] -> [300] -> [10]\n",
    "# NOTE: the \"p\" is p_drop, not p_keep\n",
    "model.add_module(\"dropout1\", torch.nn.Dropout(p=0.2))\n",
    "model.add_module(\"dense1\", torch.nn.Linear(D, 500))\n",
    "model.add_module(\"relu1\", torch.nn.ReLU())\n",
    "model.add_module(\"dropout2\", torch.nn.Dropout(p=0.5))\n",
    "model.add_module(\"dense2\", torch.nn.Linear(500, 300))\n",
    "model.add_module(\"relu2\", torch.nn.ReLU())\n",
    "model.add_module(\"dropout3\", torch.nn.Dropout(p=0.5))\n",
    "model.add_module(\"dense3\", torch.nn.Linear(300, K))\n",
    "# Note: no final softmax!\n",
    "# just like Tensorflow, it's included in cross-entropy function\n",
    "\n",
    "\n",
    "# define a loss function\n",
    "# other loss functions can be found here:\n",
    "# http://pytorch.org/docs/master/nn.html#loss-functions\n",
    "loss = torch.nn.CrossEntropyLoss(size_average=True)\n",
    "# Note: this returns a function!\n",
    "# e.g. use it like: loss(logits, labels)\n",
    "\n",
    "\n",
    "# define an optimizer\n",
    "# other optimizers can be found here:\n",
    "# http://pytorch.org/docs/master/optim.html\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "# define the training procedure\n",
    "# i.e. one step of gradient descent\n",
    "# there are lots of steps\n",
    "# so we encapsulate it in a function\n",
    "# Note: inputs and labels are torch tensors\n",
    "def train(model, loss, optimizer, inputs, labels):\n",
    "    # set the model to training mode\n",
    "    # because dropout has 2 different modes!\n",
    "    model.train()\n",
    "\n",
    "    inputs = Variable(inputs, requires_grad=False)\n",
    "    labels = Variable(labels, requires_grad=False)\n",
    "\n",
    "    # Reset gradient\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward\n",
    "    logits = model.forward(inputs)\n",
    "    output = loss.forward(logits, labels)\n",
    "\n",
    "    # Backward\n",
    "    output.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # what's the difference between backward() and step()?\n",
    "\n",
    "    return output.item()\n",
    "\n",
    "\n",
    "# similar to train() but not doing the backprop step\n",
    "def get_cost(model, loss, inputs, labels):\n",
    "    # set the model to testing mode\n",
    "    # because dropout has 2 different modes!\n",
    "    model.eval()\n",
    "\n",
    "    inputs = Variable(inputs, requires_grad=False)\n",
    "    labels = Variable(labels, requires_grad=False)\n",
    "\n",
    "    # Forward\n",
    "    logits = model.forward(inputs)\n",
    "    output = loss.forward(logits, labels)\n",
    "\n",
    "    return output.item()\n",
    "\n",
    "\n",
    "# define the prediction procedure\n",
    "# also encapsulate these steps\n",
    "# Note: inputs is a torch tensor\n",
    "def predict(model, inputs):\n",
    "    # set the model to testing mode\n",
    "    # because dropout has 2 different modes!\n",
    "    model.eval()\n",
    "\n",
    "    inputs = Variable(inputs, requires_grad=False)\n",
    "    logits = model.forward(inputs)\n",
    "    return logits.data.numpy().argmax(axis=1)\n",
    "\n",
    "\n",
    "# return the accuracy\n",
    "# labels is a torch tensor\n",
    "# to get back the internal numpy data\n",
    "# use the instance method .numpy()\n",
    "def score(model, inputs, labels):\n",
    "    predictions = predict(model, inputs)\n",
    "    return np.mean(labels.numpy() == predictions)\n",
    "\n",
    "\n",
    "### prepare for training loop ###\n",
    "\n",
    "# convert the data arrays into torch tensors\n",
    "Xtrain = torch.from_numpy(Xtrain).float()\n",
    "Ytrain = torch.from_numpy(Ytrain).long()\n",
    "Xtest = torch.from_numpy(Xtest).float()\n",
    "Ytest = torch.from_numpy(Ytest).long()\n",
    "\n",
    "# training parameters\n",
    "epochs = 15\n",
    "batch_size = 32\n",
    "n_batches = Xtrain.size()[0] // batch_size\n",
    "\n",
    "# things to keep track of\n",
    "train_costs = []\n",
    "test_costs = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "# main training loop\n",
    "for i in range(epochs):\n",
    "    cost = 0\n",
    "    test_cost = 0\n",
    "    for j in range(n_batches):\n",
    "        Xbatch = Xtrain[j * batch_size:(j + 1) * batch_size]\n",
    "        Ybatch = Ytrain[j * batch_size:(j + 1) * batch_size]\n",
    "        cost += train(model, loss, optimizer, Xbatch, Ybatch)\n",
    "\n",
    "    # we could have also calculated the train cost here\n",
    "    # but I wanted to show you that we could also return it\n",
    "    # from the train function itself\n",
    "    train_acc = score(model, Xtrain, Ytrain)\n",
    "    test_acc = score(model, Xtest, Ytest)\n",
    "    test_cost = get_cost(model, loss, Xtest, Ytest)\n",
    "\n",
    "    print(\"Epoch: %d, cost: %f, acc: %.2f\" % (i, test_cost, test_acc))\n",
    "\n",
    "    # for plotting\n",
    "    train_costs.append(cost / n_batches)\n",
    "    train_accuracies.append(train_acc)\n",
    "    test_costs.append(test_cost)\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "# plot the results\n",
    "plt.plot(train_costs, label='Train cost')\n",
    "plt.plot(test_costs, label='Test cost')\n",
    "plt.title('Cost')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_accuracies, label='Train accuracy')\n",
    "plt.plot(test_accuracies, label='Test accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785f6fa7-2c7f-4f6d-b5b8-67b22ac4305d",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "简单复习下步骤：\n",
    "\n",
    "获取数据，包括归一化处理—>构建神经网络结果，主要是定义layer，完成参数初始化—>定义forward函数，主要是每层计算输出，包括定义激活函数—>定义损失函数、优化算法—>注意要先将数据定义到张量中，然后调用函数式地调用神经网络对象以调用forward计算—>计算loss并由loss张量完成自动梯度计算反向传播—>调用优化算法的step完成参数更新—>梯度归零（也可以放在loss反向传播之前），开始新一轮循环参数更新。\n",
    "\n",
    "上述步骤中，除了重写Module之外，直接利用Sequential构建神经网络结构也是一种常见做法。\n",
    "\n",
    "接下来利用pytorch写一个神经网络的示例来自课程《Mordern deep learning in Python》。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c4d166-25da-4fa4-9c90-78928f0cd6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_normalized_data():\n",
    "    \"\"\"获取数据\"\"\"\n",
    "    print(\"Reading in and transforming data...\")\n",
    "\n",
    "    if not os.path.exists('data/mnist/train.csv'):\n",
    "        print('Looking for data/mnist/train.csv')\n",
    "        print('You have not downloaded the data and/or not placed the files in the correct location.')\n",
    "        print('Please get the data from: https://www.kaggle.com/c/digit-recognizer')\n",
    "        print('Place train.csv in the folder data adjacent to the class folder')\n",
    "        exit()\n",
    "\n",
    "    df = pd.read_csv('data/mnist/train.csv')\n",
    "    data = df.values.astype(np.float32)\n",
    "    np.random.shuffle(data)\n",
    "    X = data[:, 1:]\n",
    "    Y = data[:, 0]\n",
    "\n",
    "    Xtrain = X[:-1000]\n",
    "    Ytrain = Y[:-1000]\n",
    "    Xtest = X[-1000:]\n",
    "    Ytest = Y[-1000:]\n",
    "\n",
    "    # normalize the data\n",
    "    mu = Xtrain.mean(axis=0)\n",
    "    std = Xtrain.std(axis=0)\n",
    "    np.place(std, std == 0, 1)\n",
    "    Xtrain = (Xtrain - mu) / std\n",
    "    Xtest = (Xtest - mu) / std\n",
    "\n",
    "    return Xtrain, Xtest, Ytrain, Ytest\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch\n",
    "\n",
    "# 第一步是load data，这里采用的是deep learning里的hello world，即识别手写数字的数据集，来自kaggle。\n",
    "# get the data,\n",
    "# no need to split now, the fit() function will do it\n",
    "Xtrain, Xtest, Ytrain, Ytest = get_normalized_data()\n",
    "\n",
    "# get shapes\n",
    "_, D = Xtrain.shape\n",
    "K = len(set(Ytrain))\n",
    "\n",
    "# 第二步是构建网络结构，使用pytorch构建神经网络要比直接通过numpy手写简单很多，这里先构建一个sequential（和Keras类似），然后逐层添加网络结构即可。\n",
    "model = torch.nn.Sequential()\n",
    "# 逐层添加网络即可，在pytorch中是采用add_module()函数。第一个参数是当前层的命名，可以取任何想要的名称。第二个参数就是该层。层要么是linear transformation，要么是activation\n",
    "# 比如第一层，Linear,参数D是输入的神经元个数，第二个参数500是输出的神经元个数\n",
    "model.add_module(\"dense1\", torch.nn.Linear(D, 500))\n",
    "model.add_module(\"relu1\", torch.nn.ReLU())\n",
    "model.add_module(\"dense2\", torch.nn.Linear(500, 300))\n",
    "model.add_module(\"relu2\", torch.nn.ReLU())\n",
    "model.add_module(\"dense3\", torch.nn.Linear(300, K))\n",
    "# 第三步是构建loss函数，loss函数详情可参考http://pytorch.org/docs/master/nn.html#loss-functions\n",
    "loss = torch.nn.CrossEntropyLoss(size_average=True)\n",
    "# 第四步是设置优化函数，Adam是一种常用的优化算法，是一种改良的GD算法。算法需要神经网络的parameters作为参数。\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "# 第五步是定义训练过程和预测过程，这部分也是相对最难掌握的一部分。这部分相对TensorFlow和Theano都会麻烦一些。\n",
    "\n",
    "\n",
    "def train(model, loss, optimizer, inputs, labels):\n",
    "    \"\"\"训练过程主要包括：包装输入输出到Variable变量，初始化优化函数，前向传播，反向传播，以及参数更新，详情见每步解释\"\"\"\n",
    "    # 为什么要包装变量到Variable：把Tensor包装到Variable中，它就会开始保存所有计算历史。因此每次运算都会稍微多一些cost；另一方面，在训练循环外部对Variable进行计算操作相对容易。不包装也是可以计算的，并且后面的pytorch版本有柯南高就不需要Variable的这一步了\n",
    "    inputs = Variable(inputs, requires_grad=False)\n",
    "    labels = Variable(labels, requires_grad=False)\n",
    "\n",
    "    # pytorch的梯度计算是累计的，这对有些神经网络是比较好的，因此这里初始化为0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 直接调用model的前向函数即可得到输出\n",
    "    logits = model.forward(inputs)\n",
    "\n",
    "    # 然后计算loss\n",
    "    output = loss.forward(logits, labels)\n",
    "\n",
    "    # 接着就是进行反向传播计算\n",
    "    output.backward()\n",
    "\n",
    "    # 最后是更新参数\n",
    "    optimizer.step()\n",
    "\n",
    "    return output.item()\n",
    "\n",
    "\n",
    "def predict(model, inputs):\n",
    "    inputs = Variable(inputs, requires_grad=False)\n",
    "    logits = model.forward(inputs)\n",
    "    # argmax函数是给出axis维上数组中最大数的索引\n",
    "    return logits.data.numpy().argmax(axis=1)\n",
    "\n",
    "\n",
    "# 第六步定义各类超参数并开始训练过程\n",
    "# 首先是将numpy变量都设置为torch中的张量，注意要指定数据类型\n",
    "Xtrain = torch.from_numpy(Xtrain).float()\n",
    "Ytrain = torch.from_numpy(Ytrain).long()\n",
    "Xtest = torch.from_numpy(Xtest).float()\n",
    "\n",
    "epochs = 15\n",
    "batch_size = 32  # 每个batch的大小\n",
    "n_batches = Xtrain.size()[0] // batch_size  # batch的个数\n",
    "\n",
    "costs = []\n",
    "test_accuracies = []\n",
    "for i in range(epochs):\n",
    "    cost = 0.\n",
    "    for j in range(n_batches):\n",
    "        Xbatch = Xtrain[j * batch_size:(j + 1) * batch_size]\n",
    "        Ybatch = Ytrain[j * batch_size:(j + 1) * batch_size]\n",
    "        cost += train(model, loss, optimizer, Xbatch, Ybatch)\n",
    "\n",
    "    Ypred = predict(model, Xtest)\n",
    "    acc = np.mean(Ytest == Ypred)\n",
    "    print(\"Epoch: %d, cost: %f, acc: %.2f\" % (i, cost / n_batches, acc))\n",
    "\n",
    "    costs.append(cost / n_batches)\n",
    "    test_accuracies.append(acc)\n",
    "\n",
    "# 第七步是将训练过程可视化,# EXERCISE: plot test cost + training accuracy too。一般都是把cost和accuracy都可视化出来\n",
    "# plot the results\n",
    "plt.plot(costs)\n",
    "plt.title('Training cost')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(test_accuracies)\n",
    "plt.title('Test accuracies')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {
    "8787818f-3455-4d92-88df-ddba272e6caf.png": {
     "image/png": "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"
    },
    "f55a2c9a-b1c3-4e76-b79b-eeedfa18f7d7.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "7689d5ea-f1ca-4e8d-81c8-e581cb5a9021",
   "metadata": {},
   "source": [
    "Reading in and transforming data...\n",
    "C:\\Users\\hust2\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
    "  warnings.warn(warning.format(ret))\n",
    "Epoch: 0, cost: 0.251691, acc: 0.96\n",
    "Epoch: 1, cost: 0.117666, acc: 0.97\n",
    "Epoch: 2, cost: 0.082470, acc: 0.97\n",
    "Epoch: 3, cost: 0.054635, acc: 0.97\n",
    "Epoch: 4, cost: 0.060924, acc: 0.97\n",
    "Epoch: 5, cost: 0.052717, acc: 0.97\n",
    "Epoch: 6, cost: 0.039973, acc: 0.98\n",
    "Epoch: 7, cost: 0.034754, acc: 0.97\n",
    "Epoch: 8, cost: 0.044111, acc: 0.98\n",
    "Epoch: 9, cost: 0.050160, acc: 0.98\n",
    "Epoch: 10, cost: 0.031708, acc: 0.98\n",
    "Epoch: 11, cost: 0.042624, acc: 0.98\n",
    "Epoch: 12, cost: 0.030264, acc: 0.98\n",
    "Epoch: 13, cost: 0.032563, acc: 0.98\n",
    "Epoch: 14, cost: 0.027656, acc: 0.98\n",
    "\n",
    "![image.png](attachment:8787818f-3455-4d92-88df-ddba272e6caf.png)![image.png](attachment:f55a2c9a-b1c3-4e76-b79b-eeedfa18f7d7.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tutorial)",
   "language": "python",
   "name": "tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
